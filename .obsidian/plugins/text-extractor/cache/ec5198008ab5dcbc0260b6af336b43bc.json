{"path":"科研/低秩张量补全/Framelet Representation of Tensor Nuclear Norm for Third-Order Tensor Completion/Framelet Representation of Tensor Nuclear Norm for Third-Order Tensor Completion.pdf","text":"IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 29, 2020 7233 Framelet Representation of Tensor Nuclear Norm for Third-Order Tensor Completion Tai-Xiang Jiang , Member, IEEE, Michael K. Ng , Senior Member, IEEE, Xi-Le Zhao , Member, IEEE, and Ting-Zhu Huang Abstract— The main aim of this paper is to develop a framelet representation of the tensor nuclear norm for third-order tensor recovery. In the literature, the tensor nuclear norm can be com- puted by using tensor singular value decomposition based on the discrete Fourier transform matrix, and tensor completion can be performed by the minimization of the tensor nuclear norm which is the relaxation of the sum of matrix ranks from all Fourier transformed matrix frontal slices. These Fourier transformed matrix frontal slices are obtained by applying the discrete Fourier transform on the tubes of the original tensor. In this paper, we propose to employ the framelet representation of each tube so that a framelet transformed tensor can be constructed. Because of framelet basis redundancy, the representation of each tube is sparsely represented. When the matrix slices of the original tensor are highly correlated, we expect the corresponding sum of matrix ranks from all framelet transformed matrix frontal slices would be small, and the resulting tensor completion can be performed much better. The proposed minimization model is convex and global minimizers can be obtained. Numerical results on several types of multi-dimensional data (videos, multispectral images, and magnetic resonance imaging data) have tested and shown that the proposed method outperformed the other testing methods. Index Terms— Tensor nuclear norm, framelet, alternating direction method of multipliers (ADMM), tensor completion, tensor robust principal component analysis. I. INTRODUCTION A S a high order extension of matrix, the tensor is an important data format for multi-dimensional data appli- cations, such as color image and video processing [1]–[3], Manuscript received September 14, 2019; revised January 20, 2020; accepted May 26, 2020. Date of publication June 11, 2020; date of current version July 8, 2020. This work was supported in part by the Fundamental Research Funds for the Central Universities under Grant JBK2001011 and Grant JBK2001035, in part by the National Natural Science Foundation of China under Grant 61772003, Grant 61876203, and Grant 61702083, in part by the HKRGC GRF under Grant 12306616, Grant 12200317, Grant 12300218, and Grant 12300519, in part by the HKU under Grant 104005583, and in part by the China Postdoctoral Science Foundation under Grant 2017M610628 and Grant 2018T111031. The associate editor coordinating the review of this manuscript and approving it for publication was Dr. Jocelyn Chanussot. (Corresponding author: Xi-Le Zhao.) Tai-Xiang Jiang is with the FinTech Innovation Center, School of Economic Information Engineering, Southwestern University of Finance and Economics, Chengdu 611130, China (e-mail: taixiangjiang@gmail.com). Michael K. Ng is with the Department of Mathematics, The University of Hong Kong, Hong Kong (e-mail: mng@maths.hku.hk). Xi-Le Zhao and Ting-Zhu Huang are with the Research Center for Image and Vision Computing, School of Mathematical Sciences, University of Electronic Science and Technology of China, Chengdu 611731, China (e-mail: xlzhao122003@163.com; tingzhuhuang@126.com). Digital Object Identiﬁer 10.1109/TIP.2020.3000349 hyperspectral data recovery and fusion [4]–[6], personalized web search [7], [8], high-order web link analysis [9], magnetic resonance imaging (MRI) data recovery [10], and seismic data reconstruction [11]. Owing to the objective restrictions, for example, the imaging condition for the visual data acquir- ing and the limitation of the transmission bandwidth, the multi-dimensional data in many applications are incomplete or grossly corrupted. This motivates us to perform tensor completion [3] or tensor robust principal component analysis (RPCA) [12], in which how to characterize and utilize the internal structural information of these multidimensional data is of crucial importance. For the matrix processing, low-rank models can handle two-dimensional data of various sources [13], [14]. Gen- eralized from matrix format, a tensor is able to contain more essentially structural information, being a powerful tool for dealing with multi-modal and multi-relational data [15]. Unfortunately, it is not easy to directly extend the low-rankness from the matrix to tensors. More precisely, the deﬁnition of the tensor’s rank is still not unique. In the past decades, the most popular rank deﬁnitions are the CANDECOMP/PARAFAC (CP)-rank [16], [17] and the Tucker-rank [18], [19] (or denoted as “n-rank” in [20]). The CP-rank is based on the CP decom- position, however, computing the CP-rank of a given tensor is NP-hard [21]. The Tucker-rank is based on the Tucker decomposition, in which the tensor is unfolded along each mode unavoidably destroying the intrinsic structures of the tensor. In this paper, we investigate the newly emerged tensor rank deﬁnitions, i.e., the tensor multi-rank and the tensor tubal- rank, which are computable and induced from the tensor singular value decomposition (t-SVD). The t-SVD is initially proposed by Braman et al. [22] and Kilmer et al. [23], based on the tensor-tensor product (denoted as t-prod), in which the third-order tensors are operated integrally avoiding the loss of information inherent in matricization or ﬂattening of the tensor [24]. Meanwhile, the t-SVD has shown its superior performance in capturing the spatial-shifting corre- lation that is ubiquitous in real-world data [22], [23], [25]. Although the t-SVD is initially designed for third-order ten- sors, it has been extended to high order tensors with arbitrary dimensions [25], [26]. In [27], Kernfeld et al. note that the t-prod is based on a convolution-like operation, which can be implemented using the discrete Fourier transform (DFT). Then, given a third-order 1057-7149 © 2020 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index.html for more information. Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. 7234 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 29, 2020 tensor X ∈ Rn1×n2×n3 , its Fourier transformed (along the third mode) tensor is denoted as \u0002X ∈ Rn1×n2×n3 and its tensor multi-rank is a vector with the i -th element equal to the rank of i -th frontal slice of \u0002X [28]. The tensor nuclear norm (TNN) of X is subsequently deﬁned and it equals to the sum of the nuclear norm of \u0002X ’s frontal slices. TNN is the relaxation of the sum of matrix ranks from all \u0002X ’s slices. By minimizing the TNN, Zhang et al. [28] build the low-rank tensor completion model and provided theoretical performance bounds for third-order tensor recovery from limited sampling. Lu et al. [29] utilize the TNN1 for the tensor RPCA. Similar researches, which adopt the TNN for multi-dimensional data recovery, can be found in [30]–[32]. Other than the Fourier transform, Kernfeld et al. ﬁnd that the t-prod, together with the tensor decomposition scheme, can be deﬁned via any invertible transform, for instance, the discrete cosine transform (DCT). Namely, the t-prod can be implemented by the matrices’ product after the invertible transformation along the third mode. Xu et al. [33] validate that, when minimizing the DCT based TNN for the tensor completion problem, the DCT is superior to the DFT in terms of the preservation of the head and the tail frontal slices, because of its mirror boundary condition. Corroborative results can be found in [34], [35], which demonstrates that any invertible linear transform can be applied to induce the TNN for the tensor completion task. Coincidentally, Song et al. [36] ﬁnd that the corresponding transformed tubal-rank could be approximately smaller with an appropriate unitary transform, for instance, the Haar wavelet transform, and they prove that one can recover a low transformed tubal-rank tensor exactly with overwhelming probability provided that its transformed tubal rank is sufﬁciently small and its corrupted entries are reasonably sparse. The tensor data recovery within the t-SVD framework can be viewed as ﬁnding a low-rank approximation in the trans- formed domain. Therefore, if the transformed tensor could be approximately lower-rank, minimizing the corresponding TNN, namely the TNN deﬁned based on the transformation, would be more effective for the recovery [36]. In [34]–[36], the authors establish elegant theoretical results based on the unitary transform or the invertible linear transform. How- ever, the requirement of the invertibility prevents their results from other non-invertible (or semi-invertible) transformations, which could bring in redundancy. We note that redundancy in the transformation is important as such transformed coefﬁ- cients can contain information of missing data in the original domain, see for example the work by Cai et al. [37]. In this paper, we suggest to use the tight wavelet frame (framelet) as the transformation within the t-SVD frame- work. Because of framelet basis redundancy, the representation of each tube is sparsely represented. We expect when the matrix slices of the original tensor are correlated, the corre- sponding sum of matrix ranks from all framelet transformed matrix slices would be small. As an example, we illustrate this motivation by using magnetic resonance image (MRI) of size 142×178×121, multispectral image (MSI) of size 512×512× 1In [29], the TNN is deﬁned with a factor 1/n3. TABLE I THE MEAN VALUE OF ALL THE TRUNCATED TRANSFORMED MATRIX SLICES RANKS BY USING THE FFT AND THE FRAMELET TRANSFORM FOR MRI, MSI AND VIDEO DATA SETS 31 and video data of size 144 ×176 ×100 to demonstrate their rank reduction via the framelet transformation2 compared to the Fourier transformation. Note that for real imaging data, each transformed matrix frontal slice is not an exact low-rank matrix, but it is close to a low-rank matrix. There are many small singular values of each transformed matrix frontal slice. We show in Table I that the mean value of the matrix ranks of X (:, :, i ) (the i -th transformed matrix frontal slice). Here we discard the singular values of transformed matrix frontal slice when they are smaller than the truncation parameter, and the truncated rank of transformed matrix slice is obtained. It is clear that the mean value of such truncated matrix ranks by using framelet transformation is lower than that by using the Fourier transformation. When a framelet transformed tensor is close to a low-rank tensor compared with the use of the Fourier transform, it is expected that the resulting tensor completion can be performed much better in practice. The framelet based TNN (F-TNN) minimization models are subsequently formulated for the low-rank tensor completion (LRTC) and tensor RPCA. The proposed minimization models are convex and global minimizers can be obtained via the alternating direction multipliers method (ADMM) [38] with a theoretical convergence guarantee. We conduct numerical experiments on various types of multi-dimensional imaging data and the results verify that our framelet based method outperforms the compared methods. A. Contributions The main contributions can be summarised as follows. (i) We suggest the framelet transform within the t-SVD framework and proposed a tensor completion model, which minimizes the framelet representation of the tensor nuclear norm. (ii) To tackle the non-invertible framelet transform based models, we develop alternating direction multipliers method (ADMM) based algorithms with guaranteed con- vergence, and we test our method on various types of multi-dimensional data. The outperformance of our method further corroborates the usage of framelet. The outline of this paper is given as follows. In Section II, some preliminary background on tensors and the framelet is given. The main results, including the proposed model and algorithm, are presented in Section III. Experimental results 2The piece-wise cubic B-spline is used to generate the framelet system. Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. JIANG et al.: FRAMELET REPRESENTATION OF TNN FOR THIRD-ORDER TENSOR COMPLETION 7235 are reported in Section IV. Finally, Section V draws some conclusions. II. PRELIMINARIES This section provides the basic ingredients to induce the proposed method. We ﬁrstly give the basic tensor notations and then introduce the t-SVD framework, which has been proposed in [23], [24], [28], [29]. We restate them here at the readers’ convenience. Next, the basics of framelet are brieﬂy presented. A. Tensor Notations and Deﬁnitions Generally, a third-order tensor is denoted as X ∈ Rn1×n2×n3 , and xi, j,k is its (i, j, k)-th component. We use X (k) or X (:, :, k) to denote the k-th frontal slice of a third-order tensor X ∈ Rn1×n2×n3 . Deﬁnition 1 (Tensor Mode-3 Unfolding and Folding [39]): The mode-3 unfolding of a tensor X ∈ Rn1×n2×n3 is denoted as a matrix X(3) ∈ Rn3×n1n2 , where the tensor’s (i, j, k)-th element maps to the matrix’s (k, l)-th element satisfying l = ( j − 1)n1 + i . The mode-3 unfolding operator and its inverse are respectively denoted as unfold3 and fold3,and they satisfy X = fold3(unfold3(X )) = fold3(X(3)). Deﬁnition 2 (mode-3 tensor-matrix product [39]): The mode-3 tensor-matrix product of a tensor X ∈ Rn1×n2×n3 with a matrix A ∈ Rm×n3 is denoted by X ×3 A and is of size n1 × n2 × m.Elementwise, wehave (X ×3 A)i, j,k = n3\u0003 n=1 xi, j,n · ak,n. (1) The mode-3 tensor-matrix product can also be expressed in terms of the mode-3 unfolding Y = (X ×3 A) ⇔ Y(3) = A · unfold3(X ). The one-dimensional DFT on a vector x ∈ Rn, denoted as ¯x,is given by ¯x = Fnx ∈ Cn,where Fn ∈ Cn×n is the DFT matrix. In this paper, we use \u0002X to denote the transformed tensor by performing one-dimensional DFT along the mode-3 ﬁbers (tubes) of X . By using the DFT matrix Fn3 ∈ Cn3×n3 , we have \u0002X = X ×3 Fn3 = fold3 \u0004Fn3unfold3(X )\u0005 ∈ Cn1×n2×n3 . Deﬁnition 3 (tensor conjugate transpose [24]): The conju- gate transpose of a tensor A ∈ Cn1×n2×n3 is denoted as AH ∈ Cn2×n1×n3 , which can be obtained by conjugate transposing each of the frontal slice and then reversing the order of transposed frontal slices 2 through n3, i.e., \u0004 AH\u0005(1) = \u0004A(1)\u0005H and \u0004 AH\u0005(i) = \u0004A(n3+2−i)\u0005H (i = 2, ··· , n3). Deﬁnition 4 (t-prod [24]): The tensor-tensor-product (t- prod) C = A ∗ B of A ∈ Rn1×n2×n3 and B ∈ Rn2×n4×n3 is a tensor of size n1 × n4 × n3,where the (i, j )-th tube cij : is given by cij : = C(i, j, :) = n2\u0003 k=1 A(i, k, :) ∗ B(k, j, :) (2) where ∗ denotes the circular convolution between two tubes of same size. Fig. 1. The t-SVD of an n1 × n2 × n3 tensor. Deﬁnition 5 (identity tensor [24]): The identity tensor I ∈ Rn1×n1×n3 is the tensor whose ﬁrst frontal slice is the n1 × n1 identity matrix, and whose other frontal slices are all zeros. Deﬁnition 6 (orthogonal tensor [24]): Atensor Q ∈ Cn1×n1×n3 is orthogonal if it satisﬁes QH ∗ Q = Q ∗ QH = I. (3) Deﬁnition 7 (f-diagonal tensor [24]): Atensor A is called f-diagonal if each frontal slice A(i) is a diagonal matrix. Theorem 1 (t-SVD [23], [24]): For A ∈ Rn1×n2×n3 ,the t-SVD of A is given by A = U ∗ S ∗ V H (4) where U ∈ Rn1×n1×n3 and V ∈ Rn2×n2×n3 are orthogonal tensors, and S ∈ Rn1×n2×n3 is an f-diagonal tensor. The t-SVD is illustrated in Figure 1. Deﬁnition 8 (tensor tubal-rank and multi-rank [28]): The tubal-rank of a tensor A ∈ Rn1×n2×n3 , denoted as rankt (A), is deﬁned to be the number of non-zero singular tubes of S, where S comes from the t-SVD of A: A = U ∗ S ∗ V \u0005.That is rankt (A) = #{i : S(i, :, :) \u0006= 0}. (5) The tensor multi-rank of A ∈ Rn1×n2×n3 is a vector, denoted as rankr (A) ∈ Rn3 , with the i -th element equals to the rank of i -th frontal slice of \u0002A. Deﬁnition 9 (block diagonal form [28]): Let A denote the block-diagonal matrix of the tensor \u0002A in the Fourier domain, i.e., A ≜ blockdiag( \u0002A) ≜ ⎡ ⎢ ⎢ ⎢ ⎣ \u0002A(1) \u0002A(2) . . . \u0002A(n3) ⎤ ⎥ ⎥ ⎥ ⎦ ∈ Cn1n3×n2n3 , (6) where \u0002A(k) = \u0002A(:, :, k) is the k-th slice of \u0002A for k = 1, 2, ··· , n3. It is not difﬁcult to ﬁnd that AH = A H, i.e., the block diagonal form of a tensor’s conjugate transpose equals to the matrix conjugate transpose of the tensor’s block diagonal form. Further more, for any tensor A ∈ Rn1×n2×n3 and B ∈ Rn2×n4×n3 ,wehave A ∗ B = C ⇔ A · B = C, where · is the matrix product. Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. 7236 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 29, 2020 TABLE II TENSOR NOTATIONS Deﬁnition 10 (tensor-nuclear-norm (TNN) [28]): The ten- sor nuclear norm of a tensor A ∈ Rn1×n2×n3 , denoted as \u0007A\u0007TNN,is deﬁned as \u0007A\u0007TNN ≜ \u0007A\u0007∗, (7) where \u0007· \u0007∗ refers to the matrix nuclear norm. For a matrix X ∈ Cm×n, \u0007X\u0007∗ = \fmin{m,n} i σi ,where σi is the i -th singular value of X. The TNN can be computed via the summation of the matrix nuclear norm of Fourier transformed tensor’s slices, which are also the blocks of A.That is \u0007A\u0007TNN = n3\f i=1 \u0007 \u0002A(i)\u0007∗. We summary the frequent used notations in Table II. B. Framelet A tight frame is deﬁned as a countable set X ⊂ L2(R) with the property that ∀ f ∈ L2(R), f = \f g∈X f, g\u000b. This is equivalent to that ∀ f ∈ L2(R),wehave \u0007 f \u0007 2 L2(R) = \u0003 g∈X | f, g\u000b|2, where ·, ·\u000b is the inner product in L2(R),and \u0007· \u0007L2(R) = ·, ·\u000b 1 2 . For given \u0003 := {ψ1,ψ2, ··· ,ψr }⊂ L2(R), the afﬁne (or wavelet) system is deﬁned by the collection of the dilations and the shifts of \u0003 as X (\u0003) := {ψl, j,k : 1 ≤ l ≤ r ; j, k ∈ Z},where ψl, j,k := 2 j/2ψl (2 j · -k).When X (\u0003) forms a tight frame of L2(R), it is called a tight wavelet frame, and ψl , l = 1, 2, ··· , r are called the (tight) framelets. In the numerical scheme of image processing, the framelet transform (decomposition operator) of a vector v ∈ Rn can be represented by a matrix W ∈ Rwn×n is the framelet transform matrix constructed with n ﬁlters and l levels and w = (n − 1)l + 1. The processes of generating such matrices have been detailed in many literatures such as [37], [40]. We omit them here for readability. Then the framelet transform of a discrete signal v ∈ Rn, can be written as u = Wv ∈ Rwn. Besides, the unitary extension principle (UEP) [41] asserts that W\u0005Wv = v,where W\u0005 indicates the inverse framelet transform. However, WW\u0005u \u0006= u. III. MAIN RESULTS In this section, we replace the Fourier transform by the framelet transform. The starting point of our idea is that the framelet transform would bring in redundancy and the transformed data is of lower multi-rank. Then, we build the LRTC model and tensor RPCA model based on the framelet representation of the tensor nuclear norm and propose the ADMM based algorithms to optimize these models. A. From DFT to the Framelet Transform For a three way tensor X ∈ Rn1×n2×n3 , owing to the circular convolution in Def. 4, its t-SVD can be efﬁciently computed via the DFT. Computing the one-dimensional DFT of a vector of length n by using the DFT matrix costs O(n2),and the computational cost can be reduced to O(n log n) by employing the fast Fourier transform (FFT) technique [42]. Using the DFT matrix, for a tensor X ∈ Rn1×n2×n3 , we can obtain its Fourier transformed tensor as \u0002X = fold3 \u0004Fn3X(3)\u0005 ∈ Cn1×n2×n3 , where X(3) is the mode-3 unfolding of X Next, we will adopt the framelet transform as a substitute for the Fourier transform, and give the deﬁnition of the framelet representation of the tensor nuclear norm. For simplicity, we denote the tensor after framelet transform along the third mode as XW = fold3 \u0004 WX(3)\u0005 ∈ Rn1×n2×wn3 , where W ∈ Rwn3×n3 is the framelet transform matrix con- structed with n ﬁlters and l levels and w = (n − 1)l + 1. Considering the UEP property of the framelet transform, we have X = fold3(W\u0005[XW](3)),where [XW](3) = unfold3 (XW) . Recalling Def. 8, the tensor multi-rank is deﬁned as a vector of the ranks of the frontal slices in the Fourier transform domain. Therefore, the framelet based multi-rank is deﬁned in the same manner as follows. Deﬁnition 11 (Framelet based multi-rank): The framelet based multi-rank of a tensor X ∈ Rn1×n2×n3 is deﬁned as a vector rw ∈ Rwn3 with the i -th elements rw(i ) = rank(XW(:, :, i )) for i = 1, 2, ··· ,wn3. Here we have replaced the Fourier transform by the framelet and deﬁned the framelet based multi-rank. As mentioned before, the framelet transformed tensor can be of lower (framelet based) multi-rank. To understand this in-depth, we give some empirically numerical analyses on the singular values of the frontal slices of the transformed tensors. Here, taking the video data “news”3 as an example, the original video data is denoted as X ∈ R144×176×100 and its Fourier, DCT, and framelet transformed tensors are denoted as \u0002X , XDCT,4 and XW, respectively. In Figure 2, we exhibit the distributions of the singular values of the frontal slices of X , the Fourier transformed tensors \u0002X , the DCT transformed 3Data available at http://trace.eas.asu.edu/yuv/ 4XDCT is obtained by replacing the DFT with DCT, being similar to XW. Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. JIANG et al.: FRAMELET REPRESENTATION OF TNN FOR THIRD-ORDER TENSOR COMPLETION 7237 Fig. 2. The distribution of singular values. Here, the singular values are obtained by conducting SVD on each frontal slice of the original tensor data or the transformed tensors. tensor XDCT, and the framelet transformed tensors XW.5 In Figure 2, we show the proportion of the number of singular values of transformed matrix frontal slices in each magnitude interval. It can be found in the ﬁgure that a large proportion of the singular values of the framelet transformed data appears in the interval of [0, 10−2] compared with the original video data, the Fourier transformed tensor \u0002X , and the DCT transformed tensor XDCT. This phenomenon brings in an advantage that the data can be better approximated with lower rank via the framelet representation. In Section IV, we will illustrate tensor completion and tensor RPCA can be obtained by using the framelet representation. B. Framelet Based TNN Using the DFT matrix Fn3 , the tensor nuclear norm in (7) of a tensor X ∈ Rn1×n2×n3 can be expressed as \u0007X \u0007TNN =\u0007X \u0007∗ = n3\u0003 i=1 \u0007 \u0002X (i)\u0007∗ = n3\u0003 i=1 \u0007 \rfold3 \u0004 Fn3X(3)\u0005\u000e (:, :, k)\u0007∗, (8) where X(3) is the mode-3 unfolding of X . Deﬁnition 12 (Framelet based TNN (F-TNN)): Similarly, the framelet representation of the tensor nuclear norm can be formulated as \u0007X \u0007F-TNN =\u0007blockdiag(XW)\u0007∗ = wn3\u0003 k=1 \u0007XW(:, :, k)\u0007∗ = wn3\u0003 k=1 \u0007 \rfold3 \u0004 WX(3)\u0005\u000e (:, :, k)\u0007∗, (9) where W ∈ Rwn3×n3 is the framelet transform matrix. It is not difﬁcult to obtain that the F-TNN is a convex envelope of the \u00051 norm of the framelet based multi-rank. 5The piece-wise cubic B-spline is used to generate framelet system. C. Tensor Completion via Minimizing F-TNN Based on the proposed framelet based TNN, our tensor completion model, which is convex, is formulated as min X \u0007X \u0007F-TNN s.t. X\u0006 = O\u0006, (10) where O ∈ Rn1×n2×n3 is the incomplete observed data, and \u0006 is the set of indexes of the observed entries. X\u0006 = O\u0006 constrains that the entries of X should agree with O in \u0006. The next part gives the solving algorithm for our tensor completion model (10). Let I\u0007(X ) = \u000f 0, X ∈ \u0007, ∞, otherwise, (11) where \u0007 := {X ∈ Rn1×n2×n3 , X\u0006 = O\u0006}. Thus, the problem (10) can be rewritten as min X I\u0007(X ) + wn3\u0003 k=1 \u0007XW(:, :, k)\u0007∗ (12) Then, the minimization problem (12) can be efﬁciently solved via ADMM [38]. After introducing the auxiliary variable V ∈ Rn1×n2×wn3 , the problem (12) can be rewritten as the following unconstraint problem min X I\u0007(X ) + wn3\u0003 k=1 \u0007V(:, :, k)\u0007∗ s.t. V = XW. (13) The augmented Lagrangian function of (13) is given by Lβ (X , V,\t) = I\u0007(X ) + wn3\u0003 k=1 \u0007V(:, :, k)\u0007∗ + β 2 \u0007XW − V + \t β \u0007 2 F (14) where \t ∈ Rn1×n2×wn3 is the Lagrangian multiplier, β is the penalty parameter for the violation of the linear constraints. In the scheme of the ADMM, we update each variable alternately. V sub-problem:The V at t-th iteration is V t +1 = arg min V wn3\u0003 k=1 \u0007V(:, :, k)\u0007∗ + β 2 \u0007X t W −V + \tt β \u0007 2 F (15) Then, (15) can be decomposed into wn3 subproblems and it is easy to obtain the closed form solution of these sub-problems with the singular value thresholding (SVT) operator [43]. Hence, we update V as V t +1(:, :, k) = SVT 1 β \u0010X t W(:, :, k) + \tt (:, :, k) β \u0011 , (16) where k = 1, 2 ··· ,wn3. The complexity of computing V at each iteration is O(wn1n2n3 min(n1n2)). X sub-problem: For convenience, the subproblem of opti- mizing Lβ with respect to X at t-th iteration is written in the matrix format as (recalling that XW = fold3 \u0004 WX(3)\u0005 ) Xt +1 = arg min X I\u0007(X )+ β 2 \u0007WX−Vt +1 (3) + \t t (3) β \u0007 2 F , (17) Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. 7238 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 29, 2020 Algorithm 1 Tensor Completion via Minimizing F-TNN where Vt +1 (3) = unfold3(V t +1) and \t t (3) = unfold3(\tt ). To optimize (17), we ﬁrst solve the following equation W\u0005WX(3) = W\u0005 \u0012 Vt +1 (3) − \t t (3) β \u0013 . (18) Thus, considering that W\u0005WX(3) = X(3) (the UEP property of the framelet transformation), we have X t +1 = P\u0006C \u0012 fold3(W\u0005(Vt +1 (3) − \tt (3) β )) \u0013 +P\u0006 (O) , (19) where P\u0006(·) is the projection function that keeps the entries of · in \u0006 while making others be zeros, and \u0006c denotes the complementary set of \u0006. Meanwhile, we have X t +1 W = fold3(WXt +1 (3) ). The complexity of computing X is O(wn1n2n2 3) at each iteration. Updating the Multiplier: The multiplier \t can be updated by \t t +1 = \t t + β \u0014 X t +1 W − V t +1\u0015 . (20) Updating \t costs O(wn1n2n3) at each iteration. Finally, our algorithm is summarized in Algorithm 1. The total complexity of Algorithm 1 at each iteration is O(wn1n2n3(n3 + min(n1, n2))). The objective function of the proposed model in (10) is convex. Our algorithm ﬁts the stan- dard ADMM framework and its convergence is theoretically guaranteed [38]. D. Tensor Robust Principal Components Analysis As aforementioned, another typical tensor recovery problem is the tensor RPCA problem, which aims to recover the tensor from grossly corrupted observations. Adopting the F-TNN to characterize the low-rank part, our tensor RPCA model is formulated as min L,S \u0007L\u0007F-TNN + λ\u0007E\u00071 s.t. L + E = O, (21) where O ∈ Rn1×n2×n3 is the observed data, E indicates the sparse part, \u0007E\u00071 = \fijk |Ei, j,k |,and λ is a non-negative parameter. For convenience, we introduce an auxiliary variable V ∈ Rn1×n2×wn3 , and reformulate (22) as min L,S,V wn3\u0003 k=1 \u0007V(:, :, k)\u0007∗ + λ\u0007E\u00071 s.t. L + E = O, V = LW, (22) where LW = fold3 \u0004WL(3)\u0005 ∈ Rn1×n2×wn3 and W ∈ Rwn3×n3 is the framelet transform matrix constructed with n ﬁlters and l levels (w = (n − 1)l + 1). Similarly, we adopt ADMM to solve (22). The augmented Lagrangian function of (22) is given as Lβ (L, V, E,\t) = wn3\u0003 k=1 \u0007V(:, :, k)\u0007∗ + β 2 \u0007LW − V + \t1 β \u0007 2 F + λ\u0007E\u00071 + β 2 \u0007O − L − E + \t2 β \u0007 2 F (23) where \t1 ∈ Rn1×n2×wn3 and \t2 ∈ Rn1×n2×n3 are the Lagrangian multiplier, and β is a nonnegative parameter. In the scheme of the ADMM, we update each variable alternately as: ⎧ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨ ⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩ V t +1 = arg min V wn3\u0003 k=1 \u0007V(:, :, k)\u0007∗ + β 2 \u0007L t W −V + \t t 1 β \u0007 2 F , L t +1 = arg min L β 2 \u0007LW − V t +1 + \tt 1 β \u0007 2 F + β 2 \u0007O−L − E t + \tt 2 β \u0007 2 F , E t +1 = arg min E λ\u0007E\u00071 + β 2 \u0007O−L t +1 −E + \t t 2 β \u0007 2 F , \t t +1 1 = \t t 1 + β \u0014 L t +1 W − V t +1\u0015 , \t t +1 2 = \t t 2 + β \u0014 O − L t +1 − E t +1\u0015 . (24) Speciﬁcally, the V subproblem in (24) can be solved by V t +1(:, :, k) = SVT 1 β \u0010 L t W(:, :, k) + \t t 1(:, :, k) β \u0011 , (25) for k = 1, 2, ··· ,wn3. The complexity of updating V is O(wn1n2n3 min(n1n2)) at each iteration. The L subproblem is a least square problem and its solution can be obtained as L t +1 = 1 2 fold3 \u0012 W\u0005(Vt +1 (3) −\t t 1(3) β ) \u0013 + 1 2 \u0010 O−E t + \t t 2 β \u0011 . (26) At each iteration, computing L costs O(wn1n2n2 3).The E subproblem can be solved by E t +1 = Soft λ β \u0010 O − L t +1 + \tt 2 β \u0011 , (27) where Softτ (·) is the tensor soft-thresholding operator, and Softτ (·) = sign(·) max(|·|− τ, 0). Computing E and updating the multipliers \t1 cost O(wn1n2n3) at each iter- ation. While the computation complexity of updating \t2 is O(n1n2n3). Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. JIANG et al.: FRAMELET REPRESENTATION OF TNN FOR THIRD-ORDER TENSOR COMPLETION 7239 Algorithm 2 Tensor RPCA via Minimizing F-TNN The pseudo-code of our algorithm for tensor RPCA is summarized in Algorithm 2. At each iteration of Algorithm 2, it costs O(wn1n2n3(n3+min(n1, n2))). Likewise, Algorithm 2 ﬁts the standard ADMM framework and its convergence is theoretically guaranteed [38]. IV. NUMERICAL EXPERIMENTS In this section, to illustrate the performance of the proposed method, we will exhibit the tensor completion experimental results on three typical kinds of third-order data, i.e., the MRI data, the MSI data, and the video data. Meanwhile, we conduct Three numerical metrics, consisting of the peak signal-to- noise ratio (PSNR), the structural similarity index (SSIM) [44], and the feature similarity index (FSIM) [45] are selected to quantitatively measure the reconstructed results. On account of that the data are third-order tensors, we report the mean values of PSNR, SSIM, and FISM of all the frontal slices. Experimental Settings: We generated the framelet system via the piece-wise cubic B-spline. If not speciﬁed, the framelet decomposition level l is set as 4 (l = 2for theMSI data), and the Lagrangian penalty parameter β = 1for the tensor completion task and β = 5 when dealing with the tensor RPCA problems. The maximum iteration tmax and the convergence tolerance \f are chosen as (tmax,\f) = (100, 10−2) for the tensor completion and (tmax,\f) = (200, 10−3) for the tensor RPCA. All the methods are implemented on the platform of Windows 10 and Matlab (R2017a) with an Intel(R) Core(TM) i5-4590 CPU at 3.30GHz and 16 GB RAM. A. Tensor Completion We compare our F-TNN based tensor completion method with six methods, including a baseline low-rank matrix com- pletion (LRMC) method [46], two Tucker-rank based methods HaLRTC [3] and TMac [47], a TNN based method [48], a non-convex method minimizing the partial sum of the TNN (PSTNN) [49], the DCT based TNN method (denoted as DCTNN) [34]. When employing LRMC, the input third-order tensor data is unfolded to a matrix along the third dimension. TABLE III QUANTITATIVE COMPARISONS OF THE MRI DATA COMPLETION RESULTS BY LRMC [46], HALRTC [3], TMAC [47], TNN [48], PSTNN [49], DCTNN [34] AND THE PROPOSED METHOD.THE BEST VALUES AND THE SECOND BEST VALUES ARE RESPECTIVELY HIGH- LIGHTED BY BOLDER FONTS AND UNDERLINES 1) MRI Data: We evaluate the performance of the proposed method and the compared methods on the MRI data,6 which is of size 142 × 178 × 121. As shown in Fig. 3, this is an MRI of the brain, which consists of abundant textures of the gray matter and the white matter. The sampling rates (SR) are set as 10%, 20%, and 30%. Table III shows the quantitative assessments of the results recovered by different methods. Form Table III, it can be found that the proposed method reaches the highest indices for different sampling rates. The results by TMac and DCTNN alternatively rank the second-best place. The margins between the results by our method and the second-best results are more than 1.3dB considering the PSNR, and 0.03 for the SSIM and FSIM. We illustrate one frontal slice of the results by different methods with different random sampling rates in Fig. 3. As shown in the top row of Fig. 3, when the sampling rate is 10%, the proposed method accurately reconstructs the MRI data, with a clear margin of the gray matter and the white matter. When the sampling rate is 30%, all the methods get good performances, and the white matter regions recovered by the proposed method and TMac are the visually best. 2) MSI Data: In this subsection, we evaluate the perfor- mance of our method and the compared methods on 32 MSIs7 from the CAVE databases [50]. The size of the MSIs is 512 × 512 × 31, where the spatial resolution is 512 × 512 and the spectral resolution is 31. The sampling rates (SR) are set as 5%, 10%, and 20%.8 The average quantitative assessments of all the results by different methods are listed in Table IV. We can ﬁnd that the proposed method achieves the best performance while DCTNN obtains the second best-metrics. When the sampling rate is 20%, TMac, TNN, PSTNN, DCTNN, and the proposed method all have good performances. The third dimension of the MSI represents the spectral information and facilitates a ﬁne delivery of more faithful knowledge under real scenes [51]. Therefore, in Fig. 4, 6http://brainweb.bic.mni.mcgill.ca/brainweb/selection_normal.html 7http://www.cs.columbia.edu/CAVE/databases/multispectral/ 8For the MSI data, when the sampling rate is higher than 20%, all the methods achieve very high performances and the results are very close to the ground truths. Therefore, we select the lower sampling rates to exhibit. Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. 7240 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 29, 2020 Fig. 3. The visual illustration of the results on the MRI data by different methods with different sampling rates (SR). From left to right are the frontal slices of observed incomplete data, results by different methods and the ground truth, respectively. From top to bottom are respectively corresponding to the 106-th slice, the 110-th slice and the 115-th slice. Fig. 4. The pseudo-color images (R-1 G-2 B-31) of the completion results on the MSI data “beads” (top row), “cd” (mid row), and “clay” (bottom row) by different methods, respectively, with the sampling rate = 0.05. From left to right are the observed incomplete data, results by different methods and the ground truth, respectively. For better visualization, the intensity of the pixels are adjusted. TABLE IV THE AVERAGE PSNR, SSIM AND FSIM OF THE COMPLETION RESULTS ON 32 MSIs BY LRMC [46], HALRTC [3], TMAC [47], TNN [48], PSTNN [49], DCTNN [34] AND THE PROPOSED METHOD WITH DIFFERENT SAMPLING RATES.THE BEST VALUES AND THE SECOND BEST VALUES ARE RESPECTIVELY HIGHLIGHTED BY BOLDER FONTS AND UNDERLINES we illustrate the pseudo-color images (Red-1 Green-2 Blue- 31) of the results on the MSI data “beads”, “cd”, and “clay”, with the sampling rate = 0.05. From the similarity of the color between the results and the ground truth, we can recognize the spectral distortion. From the ﬁrst row of Fig. 4, we can see that, although DCTNN also obtains clear results on “beads” as our F-TNN, the result by DCTNN is spectrally distorted. TMac performs well on “clay”, however, undesirable artifacts can be found. The superior of the proposed F-TNN is visually obvious, considering the reconstruction of the image and preservation of spectral information. 3) Video Data: In this subsection, 9 videos9 (respectively named “foreman”, “hall”, “carphone”, “highway”, “container”, “claire”, “news”, “coastguard” and “suzie”) with the size 144 × 176 × 100 are selected as the ground truth third-order data. The contents of these videos are different, consisting of humans, roads, rivers, cars, boats, bridges, walls and so on. The scenarios in some videos (such as “foreman”, “coast- guard”, “suzie”, and “highway”) are more dynamic while in others are more static. Table V lists the average MPSNR, MSSIM, and MFSIM on these 9 videos with different sampling rates. For different sampling rates, our F-TNN obtains the results with the best quantitative metrics. When the sampling rates are 10% and 20, the performances of PSTNN and DCTNN are comparable. The DCTNN ranks second with the sampling rate of 30%. Fig. 5 exhibits the frames of the results on the videos, “news” with sampling rates 10% and 20%. The video “news” is captured 9http://trace.eas.asu.edu/yuv/ Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. JIANG et al.: FRAMELET REPRESENTATION OF TNN FOR THIRD-ORDER TENSOR COMPLETION 7241 Fig. 5. The completion results on the video data “news” different methods with different sampling rates. From left to right are the observed incomplete data, results by different methods and the ground truth, respectively. From top to bottom are respectively the 15-th frame and the 67-th frame. TABLE V THE AVERAGE PSNR, SSIM AND FSIM OF THE COMPLETION RESULTS ON 9 videos BY LRMC [46], HALRTC [3], TMAC [47], TNN [48], PSTNN [49], DCTNN [34] AND THE PROPOSED METHOD WITH DIFFERENT SAMPLING RATES.THE BEST VALUES AND THE SECOND BEST VALUES ARE RESPECTIVELY HIGHLIGHTED BY BOLDER FONTS AND UNDERLINES by a static camera in a stationary scenery, and there are two dynamic parts, which are the two newscasters in the front position and a playing screen in the back, in this video. Thus, the scenario in this video contains both dynamic and static components. Most compared methods can reconstruct the static parts well while the proposed method obtains the best recovering performances on both the two newscasters (see their faces) and the dynamic screen. To further illustrate the performance of all the methods on different videos, in Fig. 6 we exhibit the PSNR, SSIM, and FSIM on all the videos by all the methods when the sampling rate is 10%. From Fig. 6, it can be found that TMac is unstable with respect to different videos while other methods maintain better metrics when the video is more static. Although the scenario in “highway” is dynamic along the temporal direction, the contents in this video are not complicated. Therefore, many methods achieve good performances. It can be observed that the proposed method obtains the highest PSNR, SSIM, and FISM on all the videos. This validates the robustness of our F-TNN. B. Tensor Robust Principal Component Analysis In this section, we test our F-TNN based TRPCA methods on two problems, i.e., color images recovery from observations corrupted by the salt-and-pepper noise, and the background subtraction for surveillance videos. The compared methods consist of one matrix nuclear norm minimization based RPCA method (denoted as MRPCA) [14], a sum of the nuclear norm minimization based tensor RPCA method (denoted as “SNN”) [52], a TNN based tensor RPCA method [31], and a DCT Fig. 6. The PNSR, SSIM, and FSIM of the results by different methods on all the video data with the sampling rate 10%. transformed TNN based tensor RPCA method [35]. The \u00051 norm is used to characterize the sparse component by all the compared methods. The balance parameter λ, which is added to the \u00051 term, is manually selected for the best performances for all the methods. We list the settings of λ in Table VI When implementing MRPCA, we unfold the observed data O along the third mode and input O(3). For the image recovery, since that the framelet transformation matrix W requires the third dimension of the input data no less than 40, we shift the dimension of the observed image as ˆO ∈ Rn2×n3×n1 via the Matlab command “shiftdim(·,1)”. 1) Color Image Recovery: We select 4 images,10 respec- tively named “airplane”, “watch”, “fruits”, and “baboon”, 10The images named “airplane”, “fruits”, and “baboon” are of the size 512 × 512 × 3 and available at http://sipi.usc.edu/database/database.php, while the image “watch” of the size 1024 × 768 × 3 is available at https://www.sitepoint.com/mastering-image-optimization-in-wordpress/ Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. 7242 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 29, 2020 TABLE VI THE SETTINGS OF THE PARAMETER λ FOR ALL THE METHODS,GIVEN THE OBSERVATION O ∈ Rn1×n2×n3 TABLE VII QUANTITATIVE COMPARISONS OF THE IMAGE RECOVERY RESULTS OF MRPCA [14], SNN [52], TNN [31], DCTNN [35], AND THE PRO- POSED METHOD.THE BEST VALUES AND THE SECOND BEST VAL- UES ARE RESPECTIVELY HIGHLIGHTED BY BOLDER FONTS AND UNDERLINES as ground truth clean images. Then, the salt-and-pepper noise is added to these images, affecting ρ pixels. The parameter ρ varies from 5% to 10%. Table VII presents the averaged PSNR, SSIM, and FSIM values of the results by different methods for the color image recovery. We can ﬁnd that the performance of our method is the best with different ρs. We exhibit the visual results on the images “airplane” and “watch” in Fig. 7. It can be obtained that all the tensor-based methods remove the salt-and-pepper noise while the perfor- mance of MRPCA is unsatisfactory. The residual images, which are absolute values of the difference between results and clean images, are magniﬁed with a factor 2 for better visual- ization. From the residual images, we can see that our method preserves the structure and details of the color images well. 2) Background Substraction: Four video sequences, respec- tively named “Bootstrap1285”, “Escalator2805”, “Shopping- Mall1535”, and “hall1368”, are selected from Li’s dataset.11 After transforming the color frames to gray level ones, each video is of the size 130 × 160 × 40. Results by all of the methods are displayed in Fig. 8. We can see that our method and MRPCA perform well for the videos “Bootstrap1285” and “ShoppingMall1535”, while some incorrectly extractions can be found in the foreground results by other three methods, the front desk in “Bootstrap1285” and the dot pattern of the ground in “ShoppingMall1535” for examples. For videos “Escalator2805” and “hall1368”, all the methods incorrectly extract contents of the background to the foreground, more or less. Overall, the foregrounds extracted by our method are the purest. C. Discussions 1) Framelet Setting: In this part, taking the completion of MRI data (SR = 10%) as an example, we evaluate the performance of the proposed method with different Framelet 11Data available at http://vis-www.cs.umass.edu/narayana/castanza/I2Rdataset/ Fig. 7. The top four rows are the image recovery results and residual images on the image “airplane”, and the bottom 4 rows are corresponding to the image “watch”. Fig. 8. Background substraction results by different methods. The left column lists one frame of the observed video. From top to bottom are respectively separation results, i.e., the background and the foreground, of the video “Bootstrap1285”, “Escalator2805”, “ShoppingMall1535”, and “hall1368”. For better visualization, we add 0.5 to the foreground. transformation settings. Firstly, including the piece-wise cubic B-spline (denoted as “cubic”), we also adopted the Haar wavelet (denoted as “Haar”) and the piece-wise linear B-spline (denoted as “linear”) to generate the framelet transformation. Meanwhile, we also set the decomposition levels from 1 to 5. Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. JIANG et al.: FRAMELET REPRESENTATION OF TNN FOR THIRD-ORDER TENSOR COMPLETION 7243 TABLE VIII THE PSNR, SSIM AND FSIM OF THE RECOVERY RESULTS ON THE MRI DATA BY THE PROPOSED METHOD WITH DIFFERENT FRAMELET SET- TINGS.THE BEST VALUES ARE HIGHLIGHTED BY BOLDER FONTS Fig. 9. The convergence behaviours of Algorithm 1, with respect to different sampling rates and different β. The quantitative metrics of the results obtained by the pro- posed method with different framelet settings are reported in Table VIII. From Table VIII, we can ﬁnd that the piece-wise cubic B-spline is the best choice. As the decomposition level arise, the performance of the proposed method becomes better until level 5. Setting the level as 3 or 4 is a good choice. 2) Convergency Behaviours: Also, we take the completion of MRI data as an example to illustrate the convergency behaviours of our algorithm with respect to different sampling rates and different parameters. In the framework of ADMM, the parameter β, which is brought in by the augmented Lagrangian function, mainly affects the convergency behaviour of our method. Thus, we test our algorithm with β = 10−1, 1, 10. We plot \u0007V k+1 − V k\u0007∞ and \u0007X k+1 − X k\u0007∞ of each iteration in Fig. 9. It can be seen that when β = 10−1 and 1 our algorithm steadily converges. Although the behaviour of \u0007X k+1 − X k\u0007∞ is not that stable when β = 10, our algorithm also converges rapidly. V. CONCLUSION In this paper, we propose to replace the Fourier transform by the framelet in the t-SVD framework. Then, we formu- late the framelet representation of the tensor multi-rank and tensor nuclear norm. A low-rank tensor completion model and a tensor robust principal component analysis model are proposed by minimizing the framelet based tensor nuclear norm. We develop ADMM based algorithms to solve these convex models with guaranteed convergence. We compare the performance of the proposed method with state-of-the-art methods via numerical experiments on the magnetic resonance imaging data, videos, color images, and multispectral images. Our method outperforms many state-of-the-art methods quan- titatively and visually. ACKNOWLEDGMENT The authors would like to express their sincere thanks to the editor and referees for giving them so many valuable comments and suggestions for revising this paper. They also thank the support from the Financial Intelligence and Financial Engineering Research Key Laboratory. REFERENCES [1] A. Sobral and E.-H. Zahzah, “Matrix and tensor completion algorithms for background model initialization: A comparative evaluation,” Pattern Recognit. Lett., vol. 96, pp. 22–33, Sep. 2017. [2] T. Korah and C. Rasmussen, “Spatiotemporal inpainting for recovering texture maps of occluded building facades,” IEEE Trans. Image Process., vol. 16, no. 9, pp. 2262–2271, Sep. 2007. [3] J. Liu, P. Musialski, P. Wonka, and J. Ye, “Tensor completion for estimating missing values in visual data,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 1, pp. 208–220, Jan. 2013. [4] J.-H. Yang, X.-L. Zhao, T.-H. Ma, Y. Chen, T.-Z. Huang, and M. Ding, “Remote sensing images destriping using unidirectional hybrid total variation and nonconvex low-rank regularization,” J. Comput. Appl. Math., vol. 363, pp. 124–144, Jan. 2020. [5] R. Dian and S. Li, “Hyperspectral image super-resolution via subspace- based low tensor multi-rank regularization,” IEEE Trans. Image Process., vol. 28, no. 10, pp. 5135–5146, Oct. 2019. [6] L.-J. Deng, M. Feng, and X.-C. Tai, “The fusion of panchromatic and multispectral remote sensing images via tensor-based sparse modeling and hyper-Laplacian prior,” Inf. Fusion, vol. 52, pp. 76–89, Dec. 2019. [7] J.-T. Sun, H.-J. Zeng, H. Liu, Y. Lu, and Z. Chen, “CubeSVD: A novel approach to personalized Web search,” in Proc. 14th Int. Conf. World Wide Web (WWW), 2005, pp. 382–390. [8] D. A. Lima and G. M. B. Oliveira, “A cellular automata ant memory model of foraging in a swarm of robots,” Appl. Math. Model., vol. 47, pp. 551–572, Jul. 2017. [9] T. G. Kolda, B. W. Bader, and J. P. Kenny, “Higher-order Web link analysis using multilinear algebra,” in Proc. 5th IEEE Int. Conf. Data Mining (ICDM), Nov. 2005, pp. 242–249. [10] V. N. Varghees, M. S. Manikandan, and R. Gini, “Adaptive MRI image denoising using total-variation and local noise estimation,” in Proc. Int. Conf. Adv. Eng., Sci. Manage. (ICAESM), 2012, pp. 506–511. [11] N. Kreimer and M. D. Sacchi, “A tensor higher-order singular value decomposition for prestack seismic data noise reduction and interpola- tion,” Geophysics, vol. 77, no. 3, pp. V113–V122, May 2012. [12] J.-H. Yang, X.-L. Zhao, T.-Y. Ji, T.-H. Ma, and T.-Z. Huang, “Low-rank tensor train for tensor robust principal component analysis,” Appl. Math. Comput., vol. 367, Feb. 2020, Art. no. 124783. [13] E. J. Candès and B. Recht, “Exact matrix completion via convex optimization,” Commun. ACM, vol. 55, no. 6, pp. 111–119, Jun. 2012. [14] E. J. Candès, X. Li, Y. Ma, and J. Wright, “Robust principal component analysis?” J. ACM, vol. 58, no. 3, p. 11, May 2011. [15] Z. Song, D. Woodruff, and H. Zhang, “Sublinear time orthogonal tensor decomposition,” in Proc. Adv. Neural Inf. Process. Syst. (NIPS), 2016, pp. 793–801. [16] E. Acar, D. M. Dunlavy, T. G. Kolda, and M. Mørup, “Scalable tensor factorizations for incomplete data,” Chemometric Intell. Lab. Syst., vol. 106, no. 1, pp. 41–56, Mar. 2011. [17] P. Tichavský, A.-H. Phan, and A. Cichocki, “Numerical CP decom- position of some difﬁcult tensors,” J. Comput. Appl. Math., vol. 317, pp. 362–370, Jun. 2017. [18] Y.-F. Li, K. Shang, and Z.-H. Huang, “Low tucker rank tensor recovery via ADMM based on exact and inexact iteratively reweighted algo- rithms,” J. Comput. Appl. Math., vol. 331, pp. 64–81, Mar. 2018. [19] X. Li, M. K. Ng, G. Cong, Y. Ye, and Q. Wu, “MR-NTD: Manifold reg- ularization nonnegative tucker decomposition for tensor data dimension reduction and representation,” IEEE Trans. Neural Netw. Learn. Syst., vol. 28, no. 8, pp. 1787–1800, Aug. 2017. [20] S. Gandy, B. Recht, and I. Yamada, “Tensor completion and low-n-rank tensor recovery via convex optimization,” Inverse Problems, vol. 27, no. 2, Feb. 2011, Art. no. 025010. [21] C. J. Hillar and L.-H. Lim, “Most tensor problems are NP-hard,” J. ACM, vol. 60, no. 6, p. 45, Nov. 2009. [22] K. Braman, “Third-order tensors as linear operators on a space of matri- ces,” Linear Algebra Appl., vol. 433, no. 7, pp. 1241–1253, Dec. 2010. Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply. 7244 IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 29, 2020 [23] M. E. Kilmer and C. D. Martin, “Factorization strategies for third-order tensors,” Linear Algebra Appl., vol. 435, no. 3, pp. 641–658, Aug. 2011. [24] M. E. Kilmer, K. Braman, N. Hao, and R. C. Hoover, “Third-order tensors as operators on matrices: A theoretical and computational framework with applications in imaging,” SIAM J. Matrix Anal. Appl., vol. 34, no. 1, pp. 148–172, Jan. 2013. [25] C. D. Martin, R. Shafer, and B. LaRue, “An order- p tensor factorization with applications in imaging,” SIAM J. Sci. Comput., vol. 35, no. 1, pp. A474–A490, Jan. 2013. [26] Y.-B. Zheng, T.-Z. Huang, X.-L. Zhao, T.-X. Jiang, T.-H. Ma, and T.-Y. Ji, “Mixed noise removal in hyperspectral image via Low-Fibered- Rank regularization,” IEEE Trans. Geosci. Remote Sens., vol. 58, no. 1, pp. 734–749, Jan. 2020. [27] E. Kernfeld, M. Kilmer, and S. Aeron, “Tensor-tensor products with invertible linear transforms,” Linear Algebra Appl., vol. 485, pp. 545–570, Nov. 2015. [28] Z. Zhang, G. Ely, S. Aeron, N. Hao, and M. Kilmer, “Novel meth- ods for multilinear data completion and de-noising based on tensor- SVD,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2014, pp. 3842–3849. [29] C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan, “Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 5249–5257. [30] Q. Jiang and M. Ng, “Robust low-tubal-rank tensor completion via con- vex optimization,” in Proc. 28th Int. Joint Conf. Artif. Intell., Aug. 2019, pp. 10–16. [31] C. Lu, J. Feng, Y. Chen, W. Liu, Z. Lin, and S. Yan, “Tensor robust prin- cipal component analysis with a new tensor nuclear norm,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 42, no. 4, pp. 925–938, Apr. 2020. [32] W. Hu, D. Tao, W. Zhang, Y. Xie, and Y. Yang, “The twist tensor nuclear norm for video completion,” IEEE Trans. Neural Netw. Learn. Syst., vol. 28, no. 12, pp. 2961–2973, Dec. 2017. [33] W.-H. Xu, X.-L. Zhao, and M. Ng, “A fast algorithm for cosine transform based tensor singular value decomposition,” 2019, arXiv:1902.03070. [Online]. Available: http://arxiv.org/abs/1902.03070 [34] C. Lu, X. Peng, and Y. Wei, “Low-rank tensor completion with a new tensor nuclear norm induced by invertible linear transforms,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 5996–6004. [35] C. Lu and P. Zhou, “Exact recovery of tensor robust principal component analysis under linear transforms,” 2019, arXiv:1907.08288. [Online]. Available: http://arxiv.org/abs/1907.08288 [36] G. Song, M. K. Ng, and X. Zhang, “Robust tensor completion using transformed tensor singular value decomposition,” Numer. Linear Alge- bra Appl., vol. 27, no. 3, p. e2299, 2020, doi: 10.1002/nla.2299. [37] J.-F. Cai, R. H. Chan, and Z. Shen, “A framelet-based image inpainting algorithm,” Appl. Comput. Harmon. Anal., vol. 24, no. 2, pp. 131–149, Mar. 2008. [38] S. Boyd, “Distributed optimization and statistical learning via the alter- nating direction method of multipliers,” Found. Trends Mach. Learn., vol. 3, no. 1, pp. 1–122, 2010. [39] T. G. Kolda and B. W. Bader, “Tensor decompositions and applications,” SIAM Rev., vol. 51, no. 3, pp. 455–500, Aug. 2009. [40] T.-X. Jiang, T.-Z. Huang, X.-L. Zhao, T.-Y. Ji, and L.-J. Deng, “Matrix factorization for low-rank tensor completion using framelet prior,” Inf. Sci., vols. 436–437, pp. 403–417, Apr. 2018. [41] A. Ron and Z. Shen, “Afﬁne systems in L2(Rd ): The analysis of the analysis operator,” J. Funct. Anal., vol. 148, no. 2, pp. 408–447, 1997. [42] G. D. Bergland, “A guided tour of the fast Fourier transform,” IEEE Spectr., vol. 6, no. 7, pp. 41–52, Jul. 1969. [43] J.-F. Cai, E. J. Candès, and Z. Shen, “A singular value thresholding algorithm for matrix completion,” SIAM J. Optim., vol. 20, no. 4, pp. 1956–1982, Jan. 2010. [44] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality assessment: From error visibility to structural similarity,” IEEE Trans. Image Process., vol. 13, no. 4, pp. 600–612, Apr. 2004. [45] L. Zhang, L. Zhang, X. Mou, and D. Zhang, “FSIM: A feature similarity index for image quality assessment,” IEEE Trans. Image Process., vol. 20, no. 8, pp. 2378–2386, Aug. 2011. [46] E. J. Candès and B. Recht, “Exact matrix completion via convex optimization,” Found. Comput. Math., vol. 9, no. 6, p. 717, Dec. 2009. [47] Y. Xu et al., “Parallel matrix factorization for low-rank tensor comple- tion,” Inverse Problems Imag., vol. 9, no. 2, pp. 601–624, 2015. [48] Z. Zhang and S. Aeron, “Exact tensor completion using t-SVD,” IEEE Trans. Signal Process., vol. 65, no. 6, pp. 1511–1526, Mar. 2017. [49] T.-X. Jiang, T.-Z. Huang, X.-L. Zhao, and L.-J. Deng, “Multi- dimensional imaging data recovery via minimizing the partial sum of tubal nuclear norm,” J. Comput. Appl. Math., vol. 372, Jul. 2020, Art. no. 112680. [50] F. Yasuma, T. Mitsunaga, D. Iso, and S. K. Nayar, “Generalized assorted pixel camera: Postcapture control of resolution, dynamic range, and spectrum,” IEEE Trans. Image Process., vol. 19, no. 9, pp. 2241–2253, Sep. 2010. [51] Q. Xie et al., “Multispectral images denoising by intrinsic tensor sparsity regularization,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 1692–1700. [52] D. Goldfarb and Z. Qin, “Robust low-rank tensor recovery: Models and algorithms,” SIAM J. Matrix Anal. Appl., vol. 35, no. 1, pp. 225–253, Jan. 2014. Tai-Xiang Jiang (Member, IEEE) received the B.S. and Ph.D. degrees in mathematics and applied mathematics from the University of Elec- tronic Science and Technology of China (UESTC), Chengdu, China, in 2013. He is currently a Lec- turer with the School of Economic Information Engineering, Southwestern University of Finance and Economics. His research interests include sparse and low-rank modeling, tensor decompo- sition, and multi-dimensional image processing. https://sites.google.com/view/taixiangjiang/ Michael K. Ng (Senior Member, IEEE) is currently the Director of the Research Division for Mathemat- ical and Statistical Science, the Chair Professor of the Department of Mathematics, The University of Hong Kong, and the Chairman of the HKU-TCL Joint Research Center for AI. His research inter- ests include data science, scientiﬁc computing, and numerical linear algebra. Xi-Le Zhao (Member, IEEE) received the M.S. and Ph.D. degrees from the University of Electronic Sci- ence and Technology of China (UESTC), Chengdu, China, in 2009 and 2012, respectively. He is currently a Professor with the School of Mathematical Sciences, UESTC. His main research interests include models and algorithms of high-dimensional image processing problems. Ting-Zhu Huang received the B.S., M.S., and Ph.D. degrees in computational mathematics from the Department of Mathematics, Xi’an Jiaotong Uni- versity, Xi’an, China. He is currently a Professor with the School of Mathematical Sciences, University of Electronic Sci- ence and Technology of China, Chengdu, China. His research interests include scientiﬁc compu- tation and applications, numerical algorithms for image processing, numerical linear algebra, pre- conditioning technologies, and matrix analysis with applications. Dr. Huang is an Editor of The Scientiﬁc World Journal, Advances in Numerical Analysis,the Journal of Applied Mathematics,the Journal of Pure and Applied Mathematics: Advances in Applied Mathematics,and the Journal of Electronic Science and Technology, China. Authorized licensed use limited to: DALIAN MARITIME UNIVERSITY. Downloaded on September 26,2023 at 07:02:30 UTC from IEEE Xplore. Restrictions apply.","libVersion":"0.2.4","langs":""}